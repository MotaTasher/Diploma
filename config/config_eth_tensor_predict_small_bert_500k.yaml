defaults:
    - default
    - _self_


dataset_fabric: "GetEthereumDataset"

name: "model training on eth dataset v1, tensor predict, small bert"
df_config:
    start_block: 20_900_000
    end_block: 21_500_000
    # end_block: 20_930_000
    # address_limit: 10_000
    use_async: False

model: "BertForTransactionRegressionV1"

show_batch_size: 100

cnt_known_address: 2

model_params:
    # (emb_size + time_features + 1) * 2 % num_attention_heads == 0
    emb_size: 38
    num_attention_heads: 6
    # use_compositor: True
    use_compositor: False
    time_features: 45

    bert_config:
        vocab_size:             2             # текстовые токены не нужны, оставляем минимум
        hidden_size:            36            # (time_features + emb_size + 1) * 2  = (1+16+1)*2
        max_position_embeddings: 200            # позиции тоже не используем
        # num_attention_heads:    6             # 36 / 6 = 6 → целое число на голову
        intermediate_size:      144           # 4 × hidden_size
        num_hidden_layers:      4             # достаточно для 0.55 M параметров
        hidden_act:             "gelu"
        hidden_dropout_prob:    0.1
        attention_probs_dropout_prob: 0.1
        type_vocab_size:        2
        initializer_range:      0.02
        layer_norm_eps:         1e-12
        pad_token_id:           0
        position_embedding_type: "relative_key"
        use_cache:              True

learning_rate: 1e-5

gamma: 0.6
step_size: 5
warmup_epochs: 1
threshold: 0.05

num_epochs: 2_000

model_predictor: "time_cross_predictior" # time_cross_predictior 
result_loss: "result_loss_slower_change" # result_loss_slower_change, result_loss_empty

use_log: True

train_batch_size: 512
test_batch_size: 64

scheduler: reduce_on_plateau