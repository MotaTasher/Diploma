defaults:
    - default
    - _self_


dataset_fabric: "GetEthereumDataset"

name: "model training on eth dataset v1, tensor predict, small bert"
df_config:
    start_block: 20_900_000
    end_block: 21_000_000
    # end_block: 20_930_000
    # address_limit: 10_000
    use_async: False

model: "BertForTransactionRegressionV1"

show_batch_size: 100
cnt_known_address: 2

model_params:
    # (emb_size + time_features + 1) * 2 % num_attention_heads == 0
    emb_size: 38
    num_attention_heads: 2
    # use_compositor: True
    use_compositor: False
    time_features: 45

    bert_config:
        vocab_size:             5000       # <-- вместо 30522
        hidden_size:            128        # <-- вместо 768
        num_hidden_layers:      1          # <-- вместо 12
        # num_attention_heads:    2          # <-- вместо 12 (должно делиться на hidden_size)
        intermediate_size:      512        # <-- обычно = hidden_size×4
        hidden_act:             "gelu"
        hidden_dropout_prob:    0.1
        attention_probs_dropout_prob: 0.1
        max_position_embeddings: 128       # <-- вместо 512
        type_vocab_size:        2
        initializer_range:      0.02
        layer_norm_eps:         1e-12
        pad_token_id:           0
        position_embedding_type: "absolute"
        use_cache:              True

learning_rate: 1e-5

gamma: 0.6
step_size: 5
warmup_epochs: 1
threshold: 0.05

num_epochs: 2_000

model_predictor: "time_cross_predictior" # time_cross_predictior 
result_loss: "result_loss_slower_change" # result_loss_slower_change, result_loss_empty

use_log: True

train_batch_size: 1024
test_batch_size: 64

scheduler: reduce_on_plateau